import asyncio
import os
import re
import json
import psycopg2
from bs4 import BeautifulSoup
from datetime import datetime
from playwright.async_api import async_playwright


def get_db_connection():
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise Exception("‚ùå DATABASE_URL –Ω–µ –∑–∞–¥–∞–Ω–∞ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    if "sslmode" not in db_url:
        db_url += "&sslmode=require" if "?" in db_url else "?sslmode=require"
    try:
        conn = psycopg2.connect(db_url)
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        return conn
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ: {e}")
        return None


async def get_page_html(url: str) -> str:
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
            page = await context.new_page()
            await page.goto(url)
            await page.wait_for_timeout(5000)  # –∂–¥—ë–º 5 —Å–µ–∫ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
            content = await page.content()
            await browser.close()
            return content
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ Playwright: {e}")
        return ""


def create_table(conn):
    with conn.cursor() as cur:
        cur.execute("""
            CREATE TABLE IF NOT EXISTS kaspi_products (
                id SERIAL PRIMARY KEY,
                title TEXT,
                url TEXT UNIQUE,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        conn.commit()
    print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞/—Å–æ–∑–¥–∞–Ω–∞")


def extract_products_from_html(html):
    soup = BeautifulSoup(html, "html.parser")

    # DEBUG: –≤—ã–≤–æ–¥ –ø–µ—Ä–≤—ã—Ö 3000 —Å–∏–º–≤–æ–ª–æ–≤
    print("=== –ß–∞—Å—Ç—å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–µ—Ä–≤—ã–µ 3000 —Å–∏–º–≤–æ–ª–æ–≤) ===")
    print(html[:3000])
    print("=== –ö–æ–Ω–µ—Ü –≤—ã–≤–æ–¥–∞ ===")

    script_tag = soup.find("script", string=re.compile("productListData"))
    if not script_tag:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Å–∫—Ä–∏–ø—Ç —Å productListData")
        return []

    try:
        json_text_match = re.search(r'"productListData"\s*:\s*(\[\{.*?\}\])', script_tag.string, re.DOTALL)
        if not json_text_match:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON –∏–∑ —Å–∫—Ä–∏–ø—Ç–∞")
            return []

        json_data = json.loads(json_text_match.group(1))
        products = []
        for item in json_data:
            title = item.get("title")
            url = "https://kaspi.kz" + item.get("url", "")
            if title and url:
                products.append((title, url))
        return products

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ JSON: {e}")
        return []


def save_to_db(conn, products):
    with conn.cursor() as cur:
        for title, url in products:
            cur.execute(
                "INSERT INTO kaspi_products (title, url) VALUES (%s, %s) ON CONFLICT DO NOTHING",
                (title, url)
            )
        conn.commit()
    print(f"‚úÖ –ü–∞—Ä—Å–µ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω. –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
    print(f"üïí –í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {datetime.now()}")


async def main():
    print(f"üî• –ü–∞—Ä—Å–µ—Ä –∑–∞–ø—É—Å—Ç–∏–ª—Å—è: {datetime.now()}")
    conn = get_db_connection()
    if not conn:
        return

    create_table(conn)

    url = "https://kaspi.kz/shop/c/shoes/?page=1"
    html = await get_page_html(url)
    if not html:
        conn.close()
        return

    products = extract_products_from_html(html)
    if products:
        save_to_db(conn, products)
    else:
        print("‚ö†Ô∏è –ó–∞–≤–µ—Ä—à–µ–Ω–æ –±–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤")

    conn.close()


if __name__ == "__main__":
    asyncio.run(main())
